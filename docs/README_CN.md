<div align="center">

# âš¡ï¸ğŸ¤—LitHFT
**åœ¨ä½ è‡ªå·±çš„æ•°æ®ä¸Šå¯¹ä»»ä½•æ¥è‡ªhuggingfaceçš„å¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚**

| [English](https://github.com/DoubleVII/lithft) | [ä¸­æ–‡ç®€ä½“](docs/README_CN.md) |
</div>

## ä»€ä¹ˆæ˜¯LitHFT

LitHFTæ˜¯åŸºäº[LitGPT](https://github.com/Lightning-AI/litgpt)å¼€å‘çš„å·¥å…·ï¼Œç”¨äºå¯¹[Huggingface](https://huggingface.co/)ä¸­çš„è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLitGPTä¸“ä¸ºæŸäº›ç±»å‹çš„LLMsè®¾è®¡ï¼Œæ”¯æŒ20å¤šç§å¸¸ç”¨çš„LLMsã€‚ç„¶è€Œï¼Œå®ƒä¸é€‚ç”¨äºHuggingfaceä¸­çš„å…¶ä»–ç±»å‹æ¨¡å‹ï¼Œä¾‹å¦‚[Qwen](https://huggingface.co/Qwen)ã€‚LitHFTé€‚ç”¨äºåŸç”ŸHuggingfaceæ¨¡å‹ï¼Œæ— éœ€è¿›è¡Œæ£€æŸ¥ç‚¹è½¬æ¢ï¼Œä½†ä»£ä»·æ˜¯è®­ç»ƒä¼˜åŒ–è¾ƒå°‘ã€‚

|å¯¹æ¯”|LitHFT|LitGPT|
|----|----|----|
|LLMs|ä»»ä½•|20+|
|ä¼˜åŒ–|Deepspeed|FSDP|
|æ•°æ®åŠ è½½å™¨|æ¥è‡ª[TinyLLama](https://github.com/jzhang38/TinyLlama)çš„æ‰“åŒ…æ•°æ®|litdata|

## å®‰è£…LitHFT

```bash
git clone https://github.com/DoubleVII/lithft.git
cd lithft
pip install -e .
```

## è­¦å‘Š

è¯¥é¡¹ç›®ä»…åœ¨Qwenå’ŒMistralæ¨¡å‹ä¸Šæµ‹è¯•è¿‡ã€‚

|æ¨¡å‹|è®¾å¤‡|ååé‡ / s / è®¾å¤‡|
|----|----|----|
|Qwen1.5-1.8B|A800-40G|24.5k tokens|
|Mistral-7B|A100-80G|3.5k tokens|

# å¿«é€Ÿå¼€å§‹

## å¾®è°ƒ

### æ•°æ®

è¿™é‡Œæˆ‘ä»¬ä½¿ç”¨[`ParquetData`](./litgpt/data/parquet_sft_data.py)ä½œä¸ºç¤ºä¾‹ï¼ŒåŒ…å«ä¸¤åˆ—ï¼š`prompt`å’Œ`response`ã€‚ä½ å¯ä»¥ä½¿ç”¨LitGPTæ”¯æŒçš„ä»»ä½•`Datamodule`ã€‚

### å¯åŠ¨è®­ç»ƒ

```bash
huggingface-cli download "Qwen/Qwen1.5-1.8B" --local-dir Qwen1.5-1.8B --local-dir-use-symlinks False

MODEL_DIR=Qwen1.5-1.8B
DATA_PATH=sft_data.parquet # columns: `prompt` and `response`

fabric run model \
--accelerator=cuda \
--devices=8 \
launch/finetune.py \
--checkpoint_dir $MODEL_DIR \
--data ParquetData \
--data.data_path $DATA_PATH \
--train.learning_rate=1e-5 \
--train.lr_warmup_steps=100 \
--train.micro_batch_size=16 \
--train.epochs=1 \
--train.save_interval=10000 \
--train.global_batch_size=64 \
--train.log_interval=1 \
--out_dir out
```

### è½¬æ¢ä¸ºHuggingfaceæ ¼å¼

è®­ç»ƒæœŸé—´æ¨¡å‹çš„çŠ¶æ€å­—å…¸ä¸ä¼šæ”¹å˜ï¼Œç®€å•æå–å³å¯ï¼š

```bash
python3 litgpt/scripts/convert_hf_fast.py out/final/lit_model.pth/checkpoint/mp_rank_00_model_states.pt Qwen1.5-1.8B-finetuned
```

ä¿å­˜çš„æ¨¡å‹æ–‡ä»¶æ˜¯`Qwen1.5-1.8B-finetuned/pytorch_model.bin`ã€‚

ä½ å¯èƒ½éœ€è¦å¤åˆ¶ä¸€äº›å…ƒæ•°æ®åˆ°`Qwen1.5-1.8B-finetuned`ï¼Œä»¥ä¾¿é€šè¿‡`AutoModelForCausalLM`ç›´æ¥åŠ è½½æ¨¡å‹ã€‚

## é¢„è®­ç»ƒ

### æ•°æ®

å¯¹äºé¢„è®­ç»ƒï¼ŒLitHFTä½¿ç”¨æ¥è‡ª[TinyLLama](https://github.com/jzhang38/TinyLlama)ï¼ˆç»è¿‡ä¸€äº›ä¿®æ”¹ï¼‰çš„æ•°æ®åŠ è½½å™¨ã€‚ä½¿ç”¨ä»¥ä¸‹è„šæœ¬å°†`pretrain_data`ä¸‹çš„æ‰€æœ‰parquetæ–‡ä»¶å¤„ç†ä¸ºäºŒè¿›åˆ¶æ•°æ®ã€‚æ‰€æœ‰parquetæ–‡ä»¶åº”åŒ…å«åˆ—`text`ã€‚

```bash
huggingface-cli download "Qwen/Qwen1.5-1.8B" --local-dir Qwen1.5-1.8B --local-dir-use-symlinks False

python scripts/prepare_packed_data.py --source_path pretrain_data --destination_path bin/pretrain_data --tokenizer_path Qwen1.5-1.8B --prefix data_part
```

### å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

å¯¹äº`rank=0`çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ï¼Œå¯¹äºå…¶ä»–èŠ‚ç‚¹ï¼Œä½ åº”è¯¥ä¿®æ”¹`NODE_RANK`çš„å€¼ã€‚

```bash
NODE_RANK=0
MODEL_DIR=Qwen1.5-1.8B
DATA_PATH=bin/pretrain_data

fabric run model \
--node-rank=$NODE_RANK \
--main-address=$RANK0_ADDR \
--main-port=$RANK0_PORT \
--accelerator=cuda \
--devices=8 \
--num-nodes=2 \
launch/pretrain.py \
--model_config $MODEL_DIR \
--data PackedData \
--data.data_path $DATA_PATH \
--data.shuffle False \
--data.file_prefixes data_part \
--train.learning_rate 4e-4 \
--train.lr_warmup_steps=200 \
--train.micro_batch_size=3 \
--train.max_tokens=1000000000000 \
--train.save_interval=10000 \
--train.log_interval=1 \
--zero3=False \
--out_dir out
```

### è½¬æ¢ä¸ºHuggingfaceæ ¼å¼

å’Œ[å¾®è°ƒéƒ¨åˆ†](#convert-to-huggingface)ä¸€æ ·ã€‚

### ç»§ç»­è®­ç»ƒ

é€šè¿‡åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹ï¼Œå¹¶é€šè¿‡`initial_checkpoint_dir`æ ‡å¿—æŒ‡å®šæ¨¡å‹ç›®å½•æ¥æ‰§è¡Œç»§ç»­è®­ç»ƒï¼š

```bash
NODE_RANK=0
MODEL_DIR=Qwen1.5-1.8B
DATA_PATH=bin/pretrain_data

fabric run model \
--node-rank=$NODE_RANK \
--main-address=$RANK0_ADDR \
--main-port=$RANK0_PORT \
--accelerator=cuda \
--devices=8 \
--num-nodes=2 \
launch/pretrain.py \
--initial_checkpoint_dir $MODEL_DIR \
--data PackedData \
--data.data_path $DATA_PATH \
--data.shuffle False \
--data.file_prefixes data_part \
--train.learning_rate 4e-4 \
--train.lr_warmup_steps=200 \
--train.micro_batch_size=3 \
--train.max_tokens=1000000000000 \
--train.save_interval=10000 \
--train.log_interval=1 \
--zero3=False \
--out_dir out
```

## è®¸å¯è¯

LitHFTé‡‡ç”¨[MIT](https://github.com/DoubleVII/lithft/blob/main/LICENSE)è®¸å¯è¯å‘å¸ƒã€‚æ­¤å¤–ï¼Œæ­¤é¡¹ç›®è¿˜é™„å¸¦LitGPTçš„[Apache 2.0](https://github.com/Lightning-AI/litgpt/blob/main/LICENSE)è®¸å¯è¯ã€‚