<div align="center">

# âš¡ï¸ğŸ¤—LitHFT
**ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®å¯¹ä»»æ„Huggingfaceå¤§å‹è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚**

| [English](https://github.com/DoubleVII/lithft) | [ä¸­æ–‡ç®€ä½“](docs/README_CN.md) |
</div>

## ä»€ä¹ˆæ˜¯LitHFT

LitHFTæ˜¯åŸºäº[LitGPT](https://github.com/Lightning-AI/litgpt)å¼€å‘çš„å·¥å…·ï¼Œç”¨äºå¯¹[Huggingface](https://huggingface.co/)ä¸­çš„å¤§è¯­è¨€æ¨¡å‹(LLMs)è¿›è¡Œé¢„è®­ç»ƒå’Œå¾®è°ƒã€‚ç›¸æ¯”ä¹‹ä¸‹ï¼ŒLitGPTä¸“ä¸ºæŸäº›ç±»å‹çš„LLMsè®¾è®¡ï¼Œå®ƒæ”¯æŒ20å¤šç§å¸¸ç”¨çš„LLMsã€‚ç„¶è€Œï¼Œå®ƒä¸é€‚ç”¨äºHuggingfaceä¸­çš„å…¶ä»–ç±»å‹æ¨¡å‹ï¼Œä¾‹å¦‚[Qwen](https://huggingface.co/Qwen)ã€‚LitHFTé€‚ç”¨äºåŸç”ŸHuggingfaceæ¨¡å‹ï¼Œæ— éœ€å¯¹æ¨¡å‹æ£€æŸ¥ç‚¹è¿›è¡Œè½¬æ¢ï¼Œä½†ä»£ä»·æ˜¯æ›´å°‘çš„è®­ç»ƒä¼˜åŒ–ã€‚

| æ¯”è¾ƒ           |LitHFT|LitGPT|
|--------------|----|----|
| LLMs         |ä»»æ„|20+|
| Optimization |Deepspeed|FSDP|
| Dataloader   |packed data from [TinyLLama](https://github.com/jzhang38/TinyLlama)|litdata|

## å®‰è£…LitHFT

```bash
git clone https://github.com/DoubleVII/lithft.git
cd lithft
pip install -e .
```

## è­¦å‘Š

è¯¥é¡¹ç›®ä»…åœ¨Qwenå’ŒMistralæ¨¡å‹ä¸Šæµ‹è¯•è¿‡ã€‚

|æ¨¡å‹|device|ååé‡ / s / device|
|----|----|----|
|Qwen1.5-1.8B|A800-40G|24.5k tokens|
|Mistral-7B|A100-80G|3.5k tokens|

# å¿«é€Ÿå¼€å§‹

## å¾®è°ƒ

### æ•°æ®

æˆ‘ä»¬ä½¿ç”¨[`ParquetData`](../litgpt/data/parquet_sft_data.py)ä½œä¸ºç¤ºä¾‹ï¼ŒåŒ…å«ä¸¤åˆ—ï¼š`prompt`å’Œ`response`ã€‚ä½ å¯ä»¥ä½¿ç”¨ä»»ä½•LitGPTæ”¯æŒçš„`Datamodule`ã€‚

### å¯åŠ¨è®­ç»ƒ

```bash
huggingface-cli download "Qwen/Qwen1.5-1.8B" --local-dir Qwen1.5-1.8B --local-dir-use-symlinks False

MODEL_DIR=Qwen1.5-1.8B
DATA_PATH=sft_data.parquet # columns: `prompt` and `response`

fabric run model \
--accelerator=cuda \
--devices=8 \
launch/finetune.py \
--checkpoint_dir $MODEL_DIR \
--data ParquetData \
--data.data_path $DATA_PATH \
--train.learning_rate=1e-5 \
--train.lr_warmup_steps=100 \
--train.micro_batch_size=16 \
--train.epochs=1 \
--train.save_interval=10000 \
--train.global_batch_size=64 \
--train.log_interval=1 \
--out_dir out
```

### è½¬æ¢ä¸ºHuggingfaceæ ¼å¼

LitHFTçš„è®­ç»ƒä¸æ”¹å˜æ¨¡å‹çš„state dictï¼Œå› æ­¤åªéœ€è¦ç®€å•æŠ½å–å³å¯ï¼š

```bash
python3 litgpt/scripts/convert_hf_fast.py out/final/lit_model.pth/checkpoint/mp_rank_00_model_states.pt Qwen1.5-1.8B-finetuned
```

æ¨¡å‹æ–‡ä»¶ä¿å­˜åœ¨`Qwen1.5-1.8B-finetuned/pytorch_model.bin`ã€‚

ä½ å¯èƒ½éœ€è¦å¤åˆ¶ä¸€äº›å…ƒæ•°æ®åˆ°`Qwen1.5-1.8B-finetuned`ï¼Œä»¥ä¾¿é€šè¿‡`AutoModelForCausalLM`ç›´æ¥åŠ è½½æ¨¡å‹ã€‚

## é¢„è®­ç»ƒ

### æ•°æ®

å¯¹äºé¢„è®­ç»ƒï¼ŒLitHFTä½¿ç”¨æ¥è‡ª[TinyLLama](https://github.com/jzhang38/TinyLlama)ï¼ˆç»è¿‡ä¸€äº›ä¿®æ”¹ï¼‰çš„dataloaderã€‚ä½¿ç”¨ä»¥ä¸‹è„šæœ¬å°†`pretrain_data`ä¸‹çš„æ‰€æœ‰parquetæ–‡ä»¶å¤„ç†ä¸ºäºŒè¿›åˆ¶æ•°æ®ã€‚æ‰€æœ‰parquetæ–‡ä»¶åº”åŒ…å«åˆ—`text`ã€‚

```bash
huggingface-cli download "Qwen/Qwen1.5-1.8B" --local-dir Qwen1.5-1.8B --local-dir-use-symlinks False

python scripts/prepare_packed_data.py --source_path pretrain_data --destination_path bin/pretrain_data --tokenizer_path Qwen1.5-1.8B --prefix data_part
```

### å¯åŠ¨åˆ†å¸ƒå¼è®­ç»ƒ

å¯¹äºrank=0çš„èŠ‚ç‚¹ï¼Œä½¿ç”¨ä»¥ä¸‹å‘½ä»¤å¯åŠ¨ï¼Œå¯¹äºå…¶ä»–èŠ‚ç‚¹ï¼Œä½ åº”è¯¥ä¿®æ”¹`NODE_RANK`çš„å€¼ã€‚

```bash
NODE_RANK=0
MODEL_DIR=Qwen1.5-1.8B
DATA_PATH=bin/pretrain_data

fabric run model \
--node-rank=$NODE_RANK \
--main-address=$RANK0_ADDR \
--main-port=$RANK0_PORT \
--accelerator=cuda \
--devices=8 \
--num-nodes=2 \
launch/pretrain.py \
--model_config $MODEL_DIR \
--data PackedData \
--data.data_path $DATA_PATH \
--data.shuffle False \
--data.file_prefixes data_part \
--train.learning_rate 4e-4 \
--train.lr_warmup_steps=200 \
--train.micro_batch_size=3 \
--train.max_tokens=1000000000000 \
--train.save_interval=10000 \
--train.log_interval=1 \
--zero3=False \
--out_dir out
```

### è½¬æ¢ä¸ºHuggingfaceæ ¼å¼

ä¸[å¾®è°ƒ](#è½¬æ¢ä¸ºHuggingfaceæ ¼å¼)éƒ¨åˆ†ç›¸åŒã€‚

### ç»§ç»­è®­ç»ƒ

å¯ä»¥é€šè¿‡åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹æ¥æ‰§è¡Œç»§ç»­è®­ç»ƒï¼Œä½¿ç”¨`initial_checkpoint_dir`æŒ‡å®šæ¨¡å‹ç›®å½•ï¼š

```bash
NODE_RANK=0
MODEL_DIR=Qwen1.5-1.8B
DATA_PATH=bin/pretrain_data

fabric run model \
--node-rank=$NODE_RANK \
--main-address=$RANK0_ADDR \
--main-port=$RANK0_PORT \
--accelerator=cuda \
--devices=8 \
--num-nodes=2 \
launch/pretrain.py \
--initial_checkpoint_dir $MODEL_DIR \
--data PackedData \
--data.data_path $DATA_PATH \
--data.shuffle False \
--data.file_prefixes data_part \
--train.learning_rate 4e-4 \
--train.lr_warmup_steps=200 \
--train.micro_batch_size=3 \
--train.max_tokens=1000000000000 \
--train.save_interval=10000 \
--train.log_interval=1 \
--zero3=False \
--out_dir out
```

## è®¸å¯è¯

LitHFTé‡‡ç”¨[MIT](https://github.com/DoubleVII/lithft/blob/main/LICENSE)è®¸å¯è¯å‘å¸ƒã€‚æ­¤å¤–ï¼Œæ­¤é¡¹ç›®è¿˜é™„å¸¦LitGPTçš„[Apache 2.0](https://github.com/Lightning-AI/litgpt/blob/main/LICENSE)è®¸å¯è¯ã€‚